{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data/road_alpine-1_7cars_2475231.csv\n",
      "train_data/road_alpine-1_7cars_2475119.csv\n",
      "train_data/road_alpine-1_7cars_2481619.csv\n",
      "train_data/road_alpine-1_7cars_2481544.csv\n",
      "train_data/road_alpine-1_7cars_2475043.csv\n",
      "train_data/road_alpine-1_7cars_2475155.csv\n",
      "train_data/road_alpine-1_7cars_2481731.csv\n",
      "train_data/road_alpine-1_7cars_2481655.csv\n",
      "train_data/road_alpine-1_7cars_2481432.csv\n",
      "train_data/road_alpine-1_7cars_2481842.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accel</th>\n",
       "      <th>brake</th>\n",
       "      <th>steer</th>\n",
       "      <th>angle</th>\n",
       "      <th>curLapTime</th>\n",
       "      <th>distFromStart</th>\n",
       "      <th>distRaced</th>\n",
       "      <th>gear</th>\n",
       "      <th>lastLapTime</th>\n",
       "      <th>racePos</th>\n",
       "      <th>...</th>\n",
       "      <th>oppos26</th>\n",
       "      <th>oppos27</th>\n",
       "      <th>oppos28</th>\n",
       "      <th>oppos29</th>\n",
       "      <th>oppos30</th>\n",
       "      <th>oppos31</th>\n",
       "      <th>oppos32</th>\n",
       "      <th>oppos33</th>\n",
       "      <th>oppos34</th>\n",
       "      <th>oppos35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.019920e-07</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>6306.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.019920e-07</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>6306.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.019920e-07</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>6306.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.019920e-07</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>6306.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.019920e-07</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>6306.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accel  brake  steer         angle  curLapTime  distFromStart  distRaced  \\\n",
       "0    1.0    0.0    0.0  3.019920e-07      -0.982        6306.65        0.0   \n",
       "1    1.0    0.0    0.0  3.019920e-07      -0.962        6306.65        0.0   \n",
       "2    1.0    0.0    0.0  3.019920e-07      -0.942        6306.65        0.0   \n",
       "3    1.0    0.0    0.0  3.019920e-07      -0.922        6306.65        0.0   \n",
       "4    1.0    0.0    0.0  3.019920e-07      -0.902        6306.65        0.0   \n",
       "\n",
       "   gear  lastLapTime  racePos   ...     oppos26  oppos27  oppos28  oppos29  \\\n",
       "0     0          0.0        7   ...       200.0    200.0    200.0    200.0   \n",
       "1     0          0.0        7   ...       200.0    200.0    200.0    200.0   \n",
       "2     0          0.0        7   ...       200.0    200.0    200.0    200.0   \n",
       "3     0          0.0        7   ...       200.0    200.0    200.0    200.0   \n",
       "4     0          0.0        7   ...       200.0    200.0    200.0    200.0   \n",
       "\n",
       "   oppos30  oppos31  oppos32  oppos33  oppos34  oppos35  \n",
       "0    200.0    200.0    200.0    200.0    200.0    200.0  \n",
       "1    200.0    200.0    200.0    200.0    200.0    200.0  \n",
       "2    200.0    200.0    200.0    200.0    200.0    200.0  \n",
       "3    200.0    200.0    200.0    200.0    200.0    200.0  \n",
       "4    200.0    200.0    200.0    200.0    200.0    200.0  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tracknames = os.listdir(\"/home/jasper-ubuntu/Documents/Studie/Master AI/Computational intelligence/Torcs-CI/CI/train_data/\")\n",
    "# tracknames = ['aalborg.csv']\n",
    "datadictin = {}\n",
    "datadictout = {}\n",
    "datadict = {}\n",
    "framesin =[]\n",
    "framesout=[]\n",
    "data = []\n",
    "\n",
    "for i, file in  enumerate(tracknames):\n",
    "    path = os.path.join('train_data', file)\n",
    "    print(path)\n",
    "    datadictin[i] = pd.read_csv(path,sep=';', index_col=False).iloc[:-1, 3:]\n",
    "    datadictout[i] = pd.read_csv(path,sep=';', index_col=False).iloc[:-1, 0:3]\n",
    "    datadict[i]=pd.read_csv(path,sep=';', index_col=False).iloc[:,:]\n",
    "    #Todo skip\n",
    "    if datadict[i].iloc[-1,9] != 1:\n",
    "        print('Not first place' )\n",
    "    \n",
    "\n",
    "#Concatenate all datasets\n",
    "for i,file in enumerate(tracknames):\n",
    "    framesin.append(datadictin[i])\n",
    "    framesout.append(datadictout[i])\n",
    "    data.append(datadict[i])\n",
    "    \n",
    "inp = pd.concat(framesin) \n",
    "outp = pd.concat(framesout) \n",
    "train = pd.concat(data)\n",
    "\n",
    "#Drop missing values\n",
    "inp.replace('', np.nan, inplace=True)\n",
    "inp.dropna(inplace=True)\n",
    "\n",
    "outp.replace('', np.nan, inplace=True)\n",
    "outp.dropna(inplace=True)\n",
    "\n",
    "train.replace('', np.nan, inplace=True)\n",
    "train.dropna(inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331770 72 3\n"
     ]
    }
   ],
   "source": [
    "input_size = inp.shape[1]\n",
    "output_size = outp.shape[1]\n",
    "datapoints = inp.shape[0]\n",
    "print(datapoints,input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "input_size = inp.shape[1]\n",
    "hidden_size = 50\n",
    "output_size = outp.shape[1]\n",
    "num_epochs = 20\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Neural Network Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.tanh(self.fc1(x))\n",
    "        h = F.tanh(self.fc2(h))\n",
    "        h = F.tanh(self.fc3(h))\n",
    "        return h\n",
    "    \n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.tanh(self.fc1(x))\n",
    "        h = F.tanh(self.fc2(h))\n",
    "        h = F.tanh(self.fc3(h))\n",
    "        return h\n",
    "\n",
    "class RNNJens(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(RNNJens, self).__init__()\n",
    "        self.hidden_dim = H\n",
    "        \n",
    "        self.recurrent = nn.GRU(D_in, H)\n",
    "        self.linear = nn.Linear(H, D_out)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \"\"\"\n",
    "        Before we've done anything, we dont have any hidden state.\n",
    "        Refer to the Pytorch documentation to see exactly\n",
    "        why they have this dimensionality.\n",
    "        The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        return (torch.autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        lstm_out, self.hidden = self.recurrent(x, self.hidden)\n",
    "        y = self.linear(lstm_out)\n",
    "        y_pred = F.tanh(y)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def minibatch(data, batch_size=32):\n",
    "#     for i in range(0, len(data), batch_size):\n",
    "#         yield data[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable(data, labels = False):\n",
    "    \"\"\"Get a Variable given data (pandas dataframe)\"\"\"\n",
    "    # Use float (not double) for better precission: (see reply of smth)\n",
    "    # https://discuss.pytorch.org/t/problems-with-target-arrays-of-int-int32-types-in-loss-functions/140/3\n",
    "    \n",
    "    # 'Labels' or 'objective' need to be long to work (for cross entropy, for MSE they both need to be long)\n",
    "    if labels:\n",
    "        tensor = torch.from_numpy(data.iloc[:,:].as_matrix()).float()\n",
    "    else:\n",
    "        tensor = torch.from_numpy(data.iloc[:,:].as_matrix()).float()\n",
    "#     print(tensor)\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    \"\"\" ... \"\"\"\n",
    "\n",
    "    x = batch.iloc[:-1, 3:]\n",
    "    y = batch.iloc[:-1, 0:3]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNJens(input_size, hidden_size, output_size)\n",
    "# rnn_jens = (input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO learning rate scheme?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT ADAM because we have no stochastic objective function: https://arxiv.org/abs/1412.6980\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ca2166b2fee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/_functions/basic_ops.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_unexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_unexpand_or_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "for ITER in range(num_epochs):\n",
    "    \n",
    "    # Take a random sample of size batchsize\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    updates = 0\n",
    "\n",
    "    for i in range(int(datapoints/(batch_size))):\n",
    "        batch = train.sample(n = batch_size)\n",
    "        updates += 1\n",
    "\n",
    "        # pad data with zeros\n",
    "        x, y = preprocess(batch)\n",
    "        \n",
    "        # forward pass\n",
    "        scores = model(get_variable(x))\n",
    "        targets = get_variable(y, labels= True)\n",
    "        loss = nn.MSELoss()\n",
    "        output = loss(scores, targets)\n",
    "        train_loss += output.data[0]\n",
    "\n",
    "        # backward pass\n",
    "        model.zero_grad()\n",
    "        output.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"iter %r: avg train loss=%.4f, time=%.2fs\" %\n",
    "          (ITER, train_loss/updates, time.time()-start))\n",
    "\n",
    "    # evaluate\n",
    "#     _, _, acc_train = evaluate(model, train)\n",
    "#     _, _, acc_dev = evaluate(model, dev)\n",
    "#     print(\"iter %r: train acc=%.4f  test acc=%.4f\" % (ITER, acc_train, acc_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg train loss=0.0526, time=336.65s\n"
     ]
    }
   ],
   "source": [
    "# Training cell for LSTM\n",
    "for ITER in range(1):\n",
    "    \n",
    "    # Take a random sample of size batchsize\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    updates = 0\n",
    "\n",
    "    for i in range(int(datapoints/(batch_size))):\n",
    "        batch = train.sample(n = batch_size)\n",
    "        updates += 1\n",
    "        \n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # pad data with zeros\n",
    "        x, y = preprocess(batch)\n",
    "        \n",
    "        # forward pass\n",
    "        scores = model(get_variable(x))\n",
    "        targets = get_variable(y, labels= True)\n",
    "        loss = nn.MSELoss()\n",
    "        output = loss(scores, targets)\n",
    "        train_loss += output.data[0]\n",
    "\n",
    "        # backward pass\n",
    "        model.zero_grad()\n",
    "        output.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"iter %r: avg train loss=%.4f, time=%.2fs\" %\n",
    "          (ITER, train_loss/updates, time.time()-start))\n",
    "\n",
    "    # evaluate\n",
    "#     _, _, acc_train = evaluate(model, train)\n",
    "#     _, _, acc_dev = evaluate(model, dev)\n",
    "#     print(\"iter %r: train acc=%.4f  test acc=%.4f\" % (ITER, acc_train, acc_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "     0.2130     80.3540   3242.7800  ...     200.0000    200.0000    200.0000\n",
      "    -0.2783    106.4880   5923.4800  ...     200.0000    200.0000    200.0000\n",
      "    -0.0297      2.2060    150.9400  ...     200.0000    200.0000    200.0000\n",
      "                ...                   ⋱                   ...                \n",
      "     0.0571     82.4100   4065.5801  ...     200.0000    200.0000    200.0000\n",
      "     0.0670     69.7460   3424.3799  ...     200.0000    200.0000    200.0000\n",
      "     0.0503     25.9160    865.6350  ...     200.0000    200.0000    200.0000\n",
      "[torch.FloatTensor of size 99x72]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.8390  0.0279  0.0725\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.8088  0.0896  0.0939\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.8687  0.0073  0.0061\n",
      "\n",
      "(3 ,.,.) = \n",
      "  0.8680  0.0148  0.0179\n",
      "\n",
      "(4 ,.,.) = \n",
      "  0.8610  0.0258 -0.0206\n",
      "\n",
      "(5 ,.,.) = \n",
      "  0.8614  0.0249 -0.0226\n",
      "\n",
      "(6 ,.,.) = \n",
      "  0.8655 -0.0617 -0.0268\n",
      "\n",
      "(7 ,.,.) = \n",
      "  0.8554 -0.0244  0.0714\n",
      "\n",
      "(8 ,.,.) = \n",
      "  0.8601  0.0231 -0.0249\n",
      "\n",
      "(9 ,.,.) = \n",
      "  0.7345  0.0329 -0.0907\n",
      "\n",
      "(10,.,.) = \n",
      "  0.8547  0.0327  0.0159\n",
      "\n",
      "(11,.,.) = \n",
      "  0.6603 -0.0223 -0.1297\n",
      "\n",
      "(12,.,.) = \n",
      "  0.8215 -0.0867  0.0218\n",
      "\n",
      "(13,.,.) = \n",
      "  0.8592 -0.0013 -0.0387\n",
      "\n",
      "(14,.,.) = \n",
      "  0.8888 -0.0325  0.0314\n",
      "\n",
      "(15,.,.) = \n",
      "  0.8692  0.0074 -0.0464\n",
      "\n",
      "(16,.,.) = \n",
      "  0.8697  0.0066 -0.0486\n",
      "\n",
      "(17,.,.) = \n",
      "  0.8658 -0.0629 -0.0329\n",
      "\n",
      "(18,.,.) = \n",
      "  0.8696  0.0024 -0.0544\n",
      "\n",
      "(19,.,.) = \n",
      "  0.8261  0.0916  0.0729\n",
      "\n",
      "(20,.,.) = \n",
      "  0.8710  0.0037 -0.0528\n",
      "\n",
      "(21,.,.) = \n",
      "  0.8710  0.0037 -0.0528\n",
      "\n",
      "(22,.,.) = \n",
      "  0.8684  0.0111 -0.0378\n",
      "\n",
      "(23,.,.) = \n",
      "  0.8706  0.0049 -0.0505\n",
      "\n",
      "(24,.,.) = \n",
      "  0.8709  0.0039 -0.0525\n",
      "\n",
      "(25,.,.) = \n",
      "  0.8710  0.0038 -0.0528\n",
      "\n",
      "(26,.,.) = \n",
      "  0.7525  0.0415 -0.0321\n",
      "\n",
      "(27,.,.) = \n",
      "  0.7871 -0.0394  0.0104\n",
      "\n",
      "(28,.,.) = \n",
      "  0.8504  0.0166 -0.0131\n",
      "\n",
      "(29,.,.) = \n",
      "  0.8600  0.0233 -0.0217\n",
      "\n",
      "(30,.,.) = \n",
      "  0.8260  0.0916  0.0733\n",
      "\n",
      "(31,.,.) = \n",
      "  0.8616  0.0244 -0.0233\n",
      "\n",
      "(32,.,.) = \n",
      "  0.7287  0.0235 -0.1057\n",
      "\n",
      "(33,.,.) = \n",
      "  0.8128 -0.0434 -0.1052\n",
      "\n",
      "(34,.,.) = \n",
      "  0.7971  0.0421  0.0187\n",
      "\n",
      "(35,.,.) = \n",
      "  0.8494 -0.0567 -0.0301\n",
      "\n",
      "(36,.,.) = \n",
      "  0.7881 -0.0054 -0.0120\n",
      "\n",
      "(37,.,.) = \n",
      "  0.8070  0.0711  0.0771\n",
      "\n",
      "(38,.,.) = \n",
      "  0.8586  0.0220 -0.0234\n",
      "\n",
      "(39,.,.) = \n",
      "  0.8612  0.0241 -0.0233\n",
      "\n",
      "(40,.,.) = \n",
      "  0.8401 -0.0599 -0.0300\n",
      "\n",
      "(41,.,.) = \n",
      "  0.8304  0.0319  0.0242\n",
      "\n",
      "(42,.,.) = \n",
      "  0.8562  0.0283 -0.0105\n",
      "\n",
      "(43,.,.) = \n",
      "  0.8608  0.0251 -0.0213\n",
      "\n",
      "(44,.,.) = \n",
      "  0.8095  0.0483  0.0068\n",
      "\n",
      "(45,.,.) = \n",
      "  0.8776 -0.0073  0.0371\n",
      "\n",
      "(46,.,.) = \n",
      "  0.8228  0.0325 -0.0605\n",
      "\n",
      "(47,.,.) = \n",
      "  0.8304  0.0319  0.0242\n",
      "\n",
      "(48,.,.) = \n",
      "  0.8562  0.0283 -0.0105\n",
      "\n",
      "(49,.,.) = \n",
      "  0.8079  0.0475  0.0084\n",
      "\n",
      "(50,.,.) = \n",
      "  0.8079  0.0475  0.0084\n",
      "\n",
      "(51,.,.) = \n",
      "  0.8599  0.0230 -0.0246\n",
      "\n",
      "(52,.,.) = \n",
      "  0.8614  0.0242 -0.0235\n",
      "\n",
      "(53,.,.) = \n",
      "  0.8502 -0.0324 -0.0165\n",
      "\n",
      "(54,.,.) = \n",
      "  0.8601  0.0231 -0.0249\n",
      "\n",
      "(55,.,.) = \n",
      "  0.8261  0.0916  0.0729\n",
      "\n",
      "(56,.,.) = \n",
      "  0.8616  0.0244 -0.0233\n",
      "\n",
      "(57,.,.) = \n",
      "  0.8146 -0.0242  0.0242\n",
      "\n",
      "(58,.,.) = \n",
      "  0.8686 -0.0143  0.0468\n",
      "\n",
      "(59,.,.) = \n",
      "  0.7735  0.0903  0.0342\n",
      "\n",
      "(60,.,.) = \n",
      "  0.8447 -0.0895 -0.0435\n",
      "\n",
      "(61,.,.) = \n",
      "  0.8349  0.0325  0.1265\n",
      "\n",
      "(62,.,.) = \n",
      "  0.8218 -0.0336  0.0372\n",
      "\n",
      "(63,.,.) = \n",
      "  0.8375  0.0456  0.0459\n",
      "\n",
      "(64,.,.) = \n",
      "  0.8217 -0.0339  0.0372\n",
      "\n",
      "(65,.,.) = \n",
      "  0.8416  0.0210  0.0723\n",
      "\n",
      "(66,.,.) = \n",
      "  0.7858 -0.0379  0.0084\n",
      "\n",
      "(67,.,.) = \n",
      "  0.7922 -0.0194 -0.1413\n",
      "\n",
      "(68,.,.) = \n",
      "  0.8176  0.0299  0.0351\n",
      "\n",
      "(69,.,.) = \n",
      "  0.8493  0.0507  0.0205\n",
      "\n",
      "(70,.,.) = \n",
      "  0.8375  0.0456  0.0459\n",
      "\n",
      "(71,.,.) = \n",
      "  0.8113  0.1283  0.0060\n",
      "\n",
      "(72,.,.) = \n",
      "  0.8375  0.0456  0.0459\n",
      "\n",
      "(73,.,.) = \n",
      "  0.7998  0.0287  0.0918\n",
      "\n",
      "(74,.,.) = \n",
      "  0.8764 -0.0082  0.0388\n",
      "\n",
      "(75,.,.) = \n",
      "  0.7593  0.0465 -0.0296\n",
      "\n",
      "(76,.,.) = \n",
      "  0.8010 -0.0200 -0.0573\n",
      "\n",
      "(77,.,.) = \n",
      "  0.8499  0.0145 -0.0322\n",
      "\n",
      "(78,.,.) = \n",
      "  0.6095  0.1216 -0.0137\n",
      "\n",
      "(79,.,.) = \n",
      "  0.8304  0.0319  0.0242\n",
      "\n",
      "(80,.,.) = \n",
      "  0.8562  0.0283 -0.0105\n",
      "\n",
      "(81,.,.) = \n",
      "  0.8585  0.0313 -0.0085\n",
      "\n",
      "(82,.,.) = \n",
      "  0.7451  0.0223 -0.0765\n",
      "\n",
      "(83,.,.) = \n",
      "  0.7752  0.0109 -0.0961\n",
      "\n",
      "(84,.,.) = \n",
      "  0.8499  0.0145 -0.0323\n",
      "\n",
      "(85,.,.) = \n",
      "  0.8599  0.0230 -0.0247\n",
      "\n",
      "(86,.,.) = \n",
      "  0.8587  0.0314 -0.0087\n",
      "\n",
      "(87,.,.) = \n",
      "  0.7575  0.0512 -0.0175\n",
      "\n",
      "(88,.,.) = \n",
      "  0.7872 -0.0396  0.0107\n",
      "\n",
      "(89,.,.) = \n",
      "  0.8581  0.0059  0.0252\n",
      "\n",
      "(90,.,.) = \n",
      "  0.8596  0.0243 -0.0197\n",
      "\n",
      "(91,.,.) = \n",
      "  0.8628  0.0704  0.1411\n",
      "\n",
      "(92,.,.) = \n",
      "  0.8599  0.0230 -0.0246\n",
      "\n",
      "(93,.,.) = \n",
      "  0.8614  0.0242 -0.0235\n",
      "\n",
      "(94,.,.) = \n",
      "  0.8538  0.0399  0.0363\n",
      "\n",
      "(95,.,.) = \n",
      "  0.7640 -0.0209 -0.0422\n",
      "\n",
      "(96,.,.) = \n",
      "  0.8227 -0.0884  0.0240\n",
      "\n",
      "(97,.,.) = \n",
      "  0.8826 -0.0431  0.0385\n",
      "\n",
      "(98,.,.) = \n",
      "  0.8679  0.0063 -0.0454\n",
      "[torch.FloatTensor of size 99x1x3]\n",
      "\n",
      "          accel     brake     steer\n",
      "3891   0.082153  0.000000 -0.012715\n",
      "23331  0.758367  0.000000 -0.091118\n",
      "12415  1.000000  0.000000 -0.001383\n",
      "14060  1.000000  0.000000  0.047961\n",
      "28459  1.000000  0.000000  0.044378\n",
      "28383  1.000000  0.000000  0.014180\n",
      "10929  1.000000  0.000000 -0.061541\n",
      "7069   1.000000  0.000000  0.035299\n",
      "27114  1.000000  0.000000  0.039845\n",
      "2158   0.000000  0.643393  0.022996\n",
      "5607   1.000000  0.000000 -0.015162\n",
      "93     0.948170  0.000000  0.000780\n",
      "12228  1.000000  0.000000  0.001864\n",
      "31951  1.000000  0.000000 -0.017885\n",
      "17455  1.000000  0.000000 -0.019565\n",
      "25146  0.732874  0.000000 -0.479389\n",
      "29528  1.000000  0.000000 -0.081984\n",
      "11514  1.000000  0.000000  0.047461\n",
      "18524  1.000000  0.000000  0.022217\n",
      "28792  1.000000  0.000000  0.069577\n",
      "32428  1.000000  0.000000  0.035402\n",
      "31173  1.000000  0.000000 -0.004379\n",
      "22749  1.000000  0.000000  0.077770\n",
      "13592  0.029931  0.000000  0.006359\n",
      "18151  1.000000  0.000000 -0.000488\n",
      "26963  0.081123  0.000000  0.060222\n",
      "2624   1.000000  0.000000 -0.001605\n",
      "9327   1.000000  0.000000  0.060808\n",
      "32152  1.000000  0.000000  0.011983\n",
      "20685  1.000000  0.000000  0.023859\n",
      "...         ...       ...       ...\n",
      "5723   1.000000  0.000000  0.059238\n",
      "14233  0.359748  0.000000  0.470428\n",
      "4439   1.000000  0.000000  0.014122\n",
      "30265  1.000000  0.000000  0.003482\n",
      "12123  1.000000  0.000000  0.122161\n",
      "17017  1.000000  0.000000 -0.241968\n",
      "1472   1.000000  0.000000 -0.000401\n",
      "32699  1.000000  0.000000  0.232955\n",
      "22516  0.408245  0.000000 -0.010889\n",
      "118    1.000000  0.000000 -0.416194\n",
      "14826  0.269924  0.000000 -0.300248\n",
      "25174  1.000000  0.000000  0.089328\n",
      "22800  1.000000  0.000000 -0.033691\n",
      "2109   1.000000  0.000000  0.004647\n",
      "13586  0.000000  0.117698  0.013217\n",
      "29677  1.000000  0.000000  0.056488\n",
      "18104  1.000000  0.000000 -0.002792\n",
      "17390  1.000000  0.000000  0.008105\n",
      "2275   0.012380  0.000000  0.066035\n",
      "9109   0.122736  0.000000 -0.231567\n",
      "14387  1.000000  0.000000  0.022756\n",
      "22315  1.000000  0.000000 -0.033332\n",
      "7106   1.000000  0.000000  0.011466\n",
      "29944  1.000000  0.000000 -0.018466\n",
      "26685  1.000000  0.000000 -0.050561\n",
      "5862   1.000000  0.000000 -0.033980\n",
      "2590   1.000000  0.000000 -0.027505\n",
      "10587  1.000000  0.000000 -0.025473\n",
      "15792  1.000000  0.000000 -0.056355\n",
      "30219  1.000000  0.000000  0.015426\n",
      "\n",
      "[99 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Test prediction\n",
    "testval = get_variable(x)\n",
    "print(testval)\n",
    "print(model(testval))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasper-ubuntu/miniconda3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "save_path_model = 'simple_ff_model_20epochs_batch100.pt'\n",
    "save_path_dict = 'simple_ff_DICT_20epochs_batch100_dict_hiden_size50_2HL_INP72_outp3_Forward_tanh_tanh_tanh.pt'\n",
    "# https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "torch.save(model, save_path_model)\n",
    "torch.save(model.state_dict(),save_path_dict)\n",
    "# model.save_state_dict('trainedmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
