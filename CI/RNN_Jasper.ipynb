{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as t_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data/road_alpine-1_7cars_2475231.csv\n",
      "train_data/road_alpine-1_7cars_2475119.csv\n",
      "train_data/road_alpine-1_7cars_2475043.csv\n",
      "train_data/road_alpine-1_7cars_2475155.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accel</th>\n",
       "      <th>brake</th>\n",
       "      <th>steer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accel  brake  steer\n",
       "0    1.0    0.0    0.0\n",
       "1    1.0    0.0    0.0\n",
       "2    1.0    0.0    0.0\n",
       "3    1.0    0.0    0.0\n",
       "4    1.0    0.0    0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tracknames = os.listdir(\"/home/jasper-ubuntu/Documents/Studie/Master AI/Computational intelligence/Torcs-CI/CI/train_data/\")\n",
    "# tracknames = ['aalborg.csv']\n",
    "datadictin = {}\n",
    "datadictout = {}\n",
    "framesin =[]\n",
    "framesout=[]\n",
    "for i, file in  enumerate(tracknames):\n",
    "    path = os.path.join('train_data', file)\n",
    "    print(path)\n",
    "    datadictin[i] = pd.read_csv(path,sep=';', index_col=False).iloc[:-1, 3:]\n",
    "    datadictout[i] = pd.read_csv(path,sep=';', index_col=False).iloc[:-1, 0:3]\n",
    "\n",
    "\n",
    "#Concatenate all datasets\n",
    "for i,file in enumerate(tracknames):\n",
    "    framesin.append(datadictin[i])\n",
    "    framesout.append(datadictout[i])\n",
    "    \n",
    "inp = pd.concat(framesin) \n",
    "outp = pd.concat(framesout) \n",
    "\n",
    "outp.head()\n",
    "\n",
    "#Drop missing values\n",
    "inp.replace('', np.nan, inplace=True)\n",
    "inp.dropna(inplace=True)\n",
    "outp.replace('', np.nan, inplace=True)\n",
    "outp.dropna(inplace=True)\n",
    "outp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132708 72 3\n"
     ]
    }
   ],
   "source": [
    "inp_size = inp.shape[1]\n",
    "outp_size = outp.shape[1]\n",
    "datapoints = inp.shape[0]\n",
    "print(datapoints,inp_size,outp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "     0.0000     -0.9820   6306.6499  ...     200.0000    200.0000    200.0000\n",
      "     0.0000     -0.9620   6306.6499  ...     200.0000    200.0000    200.0000\n",
      "     0.0000     -0.9420   6306.6499  ...     200.0000    200.0000    200.0000\n",
      "                ...                   ⋱                   ...                \n",
      "    -0.0359     90.9260   2000.6600  ...     200.0000    200.0000    200.0000\n",
      "    -0.0366     90.9480   2000.9600  ...     200.0000    200.0000    200.0000\n",
      "    -0.0363     90.9700   2001.2700  ...     200.0000    200.0000    200.0000\n",
      "[torch.FloatTensor of size 132708x72]\n",
      "\n",
      "Variable containing:\n",
      " 1.0000  0.0000  0.0000\n",
      " 1.0000  0.0000  0.0000\n",
      " 1.0000  0.0000  0.0000\n",
      "           ⋮            \n",
      " 1.0000  0.0000 -0.0226\n",
      " 1.0000  0.0000 -0.0759\n",
      " 1.0000  0.0000 -0.0557\n",
      "[torch.FloatTensor of size 132708x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Get data from pandas in a Tensor Variable\n",
    "# x = Variable(torch.from_numpy(inp.iloc[:,:].as_matrix()).float())\n",
    "# y = Variable(torch.from_numpy(outp.iloc[:,:].as_matrix()).float())\n",
    "# print(x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Net Classes\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, outp_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states \n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.lstm(x, (h0, c0))  \n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.inp = torch.nn.Linear(25, hidden_size)\n",
    "        self.rnn = torch.nn.LSTM(hidden_size, hidden_size, 2, dropout=0.05)\n",
    "        self.out = torch.nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def step(self, input, hidden=None):\n",
    "        #print(input.view(1, -1).unsqueeze(1))\n",
    "        input = self.inp(input.view(1, -1)).unsqueeze(1)\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.out(output.squeeze(1))\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, inputs, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(inputs)\n",
    "        outputs = Variable(torch.zeros(steps, 3, 1))\n",
    "        for i in range(steps):\n",
    "            if force or i == 0:\n",
    "                input = inputs[i]\n",
    "            else:\n",
    "                input = output\n",
    "            output, hidden = self.step(input, hidden)\n",
    "            outputs[i] = output\n",
    "        return outputs, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 4 required positional arguments: 'hidden_size', 'output_size', 'batch_size', and 'num_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-643d3660d6c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Forward + Backward + Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 4 required positional arguments: 'hidden_size', 'output_size', 'batch_size', and 'num_layers'"
     ]
    }
   ],
   "source": [
    "#Hyper params\n",
    "sequence_length = 1\n",
    "input_size = inp.shape[1]\n",
    "hidden_size = 50\n",
    "num_layers = 3\n",
    "output_size = outp.shape[1]\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Construct dataset variables\n",
    "x = torch.from_numpy(inp.iloc[:,:].as_matrix()).float()\n",
    "y = torch.from_numpy(outp.iloc[:,:].as_matrix()).float()\n",
    "\n",
    "data_set = torch.utils.data.TensorDataset(x, y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize model instance\n",
    "\n",
    "model = LSTM(input_size, hidden_size, output_size, batch_size, num_layers)\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, sequence_length, input_size))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = LSTM(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, i+1, len(sensors), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'all_tracks.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f7b46fd7432f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# Read in file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all_tracks.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data length: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f7b46fd7432f>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mpd_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_tracks.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mdata_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'all_tracks.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd\n",
    "from random import shuffle\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time, math\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "class simpleGRU(nn.Module):\n",
    "    def __init__(self, D_in, h_layer_size, n_hidden_layers, D_out):\n",
    "        super(simpleGRU, self).__init__()\n",
    "        self.D_in = D_in\n",
    "        self.h_layer_size = h_layer_size\n",
    "        self.D_out = D_out\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "\n",
    "        self.encoder = nn.Linear(D_in, h_layer_size)\n",
    "        # self.encoder = nn.Embedding(D_in, h_layer_size)\n",
    "        self.gru = nn.GRU(h_layer_size, h_layer_size, n_hidden_layers)\n",
    "        self.decoder = nn.Linear(h_layer_size, D_out)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        encoded = self.encoder(input.view(1, -1))\n",
    "        out, hidden = self.gru(encoded.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(out.view(1, -1))\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_hidden_layers, 1, self.h_layer_size))\n",
    "\n",
    "\n",
    "def random_train(seq_length):\n",
    "    start_ix = random.randint(0, data_size - seq_length)\n",
    "    end_ix = start_ix + seq_length + 1\n",
    "    sequence = data[start_ix:end_ix]\n",
    "    \n",
    "    inp = char_tensor(sequence[:-1])\n",
    "    target = char_tensor(sequence[1:])\n",
    "    return inp, target\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(seq_length):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c][0:3].view(1, -1))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / seq_length\n",
    "\n",
    "def read_data(filename):\n",
    "    pd_data = pd.read_csv('all_tracks.csv')\n",
    "    data = torch.from_numpy(pd_data.values).type(torch.FloatTensor)\n",
    "    data_size = data.shape[0]\n",
    "    D_in = data.shape[1]\n",
    "    return data, data_size, D_in\n",
    "\n",
    "def get_train_pair(data, start_ix, seq_length):\n",
    "    end_ix = start_ix + seq_length + 1\n",
    "    sequence = data[start_ix:end_ix]\n",
    "\n",
    "    inp = Variable(sequence[:-1])\n",
    "    target = Variable(sequence[1:])\n",
    "\n",
    "    return inp, target\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1)\n",
    "\n",
    "    seq_length = 25 # number of steps to unroll the RNN for\n",
    "    hidden_size = 25\n",
    "    n_hidden_layers = 3\n",
    "    D_out = 3\n",
    "    lr = 0.005\n",
    "\n",
    "    n_epochs = 100\n",
    "    print_every = 1\n",
    "    plot_every = 1\n",
    "    save_every = 1\n",
    "\n",
    "    # Read in file\n",
    "    filename = 'all_tracks.csv'\n",
    "    data, data_size, D_in = read_data(filename)\n",
    "    print('Data length: %d' % data_size)\n",
    "\n",
    "    # Setup network\n",
    "    decoder = simpleGRU(D_in, hidden_size, n_hidden_layers, D_out)\n",
    "    # decoder.load_state_dict(torch.load('simpleGRU_epoch_2000file_sherlock.txt.pkl'))\n",
    "\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    start = time.time()\n",
    "    all_losses = []\n",
    "    loss_avg = 0\n",
    "\n",
    "    print('Starting training: Total Epochs: %d' % (n_epochs))\n",
    "    # Start training\n",
    "    for epoch in range(n_epochs):        \n",
    "        counter = 0\n",
    "        # Create random indexes for training sequences\n",
    "        total_samples = data_size // (seq_length + 1)\n",
    "        sample_list = list(range(total_samples))\n",
    "        shuffle(sample_list)\n",
    "\n",
    "        for ix in sample_list:\n",
    "            inp, target = get_train_pair(data, ix, seq_length)\n",
    "            # print(inp, target)\n",
    "            # sys.exit()\n",
    "\n",
    "            loss = train(inp, target)\n",
    "            loss_avg += loss\n",
    "            counter += 1\n",
    "            if counter % 500 == 0:\n",
    "                print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print('[Finished Epoch %s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "            # print(evaluate('I ', predict_len=100, temperature=0.4), '\\n')\n",
    "            \n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            all_losses.append(loss_avg / plot_every)\n",
    "            loss_avg = 0\n",
    "        \n",
    "        if epoch % save_every == 0:\n",
    "            torch.save(decoder.state_dict(), 'simpleGRU_epoch_' + str(epoch) + '_' + filename + '.pkl')\n",
    "            print(\"Model saved, epoch: %d\" % (epoch))\n",
    "\n",
    "    f= open(\"all_losses.txt\",\"w+\")\n",
    "    for i in range(len(all_losses)):\n",
    "        f.write(str(all_losses[i]) + ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
